# This workflow is intended to be run with the --sdm=conda option,
# and most rules require either the sc2ts or pangolin environments.
# The base conda environment that snakemake is run from requires
# snakemake, tszip, pandas and tqdm in order to run the "run"
# rules (which cannot be used with "conda")

rule all:
    input:
        # Base debug report
        "logs/sc2ts_v1_2023-02-21_debug-report.ipynb",
        # Final ARG
        # Order of operations:
        # 1) postprocess (various minor tweaks)
        # 2) Dating (before adding lots of highly recurrent sites)
        # 3) Parsimony mapping (adding in deletions and recurrent sites)
        # 4) Shift breakpoints (must be after mapping)
        # 5) Compute pango assignment (must be after parsimony mapping)
        # 6) Minimise metadata (must be last step)
        "sc2ts_v1_2023-02-21_pp_dated_remapped_bps_pango_mmps.trees.tsz",
        # Recombinant data
        "sc2ts_v1_2023-02-21_pp_dated_remapped_bps_pango_recombinants.csv", 
        # TODO add rule to generate recombinants_matches_pangonet csv
        # once we've sorted the reruns json.
        "sc2ts_v1_2023-02-21_pp_dated_remapped_bps_pango_recombinants_matches_pangonet_nsl.csv",
        # "sc2ts_v1_2023-02-21_pp_dated_remapped_bps_pango_rematch.json",
        # Report on deletion and parsimony mapping
        "logs/sc2ts_v1_2023-02-21_pp_dated_remapped_deletion-report.ipynb",
        # Usher tree and sc2ts intersections
        # Full usher tree with mutations, but uncalibrated node time
        "usher_v1_2024-06-06_mm.trees.tsz",
        # Usher intersection with sc2ts, with dated nodes
        "usher_2023-02-21_intersection_ds_di.trees.tsz",
        # Sc2ts intersection, not dated (annoying metdata issues)
        "sc2ts_2023-02-21_intersection.trees.tsz",
        # All-sites parsimony files, to get the direct comparison
        "usher_2023-02-21_intersection_asp.trees",
        "sc2ts_2023-02-21_intersection_asp.trees",
        # Report
        "logs/usher_sc2ts_comparison.ipynb",


rule initialise:
    input:
        "../inference/results/v1-beta1/v1-beta1_{DATE}.ts",
    output:
        "sc2ts_v1_{DATE}.trees",
    shell:
        """
        cp {input} {output}
        """

rule debug_report:
    conda:
        "sc2ts.yml"
    input:
        "{DR_PREFIX}.trees",
    log:
        notebook="logs/{DR_PREFIX}_debug-report.ipynb"
    notebook:
        "notebooks/debug_report.py.ipynb"


# Add exact matches to the dataset and do some minor mutation
# rearrangements like pushing unary recombinant mutations.
rule postprocess:
    conda:
        "sc2ts.yml"
    input:
        "{PP_PREFIX}.trees"
    output:
        "{PP_PREFIX}_pp.trees",
    shell:
        """
        python -m sc2ts postprocess {input} {output} \
            --match-db=../inference/results/v1-beta1.matches.db
        """

# NOTE: this rule should be renamed to be more general if we 
# do map sites not just for dels but things we omit also
rule remap_sites:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{MD_PREFIX}.trees",
        "remapped_sites.txt",
    output:
        "{MD_PREFIX}_remapped.trees",
        "{MD_PREFIX}_remapped_parsimony.csv",
    shell:
        """
        python -m sc2ts map-parsimony -vv \
            {input[0]} {input[1]} {output[0]} \
            --sites={input[2]} --report={output[1]} --no-progress
        """

rule deletion_report:
    conda:
        "sc2ts.yml"
    input:
        "{DELR_PREFIX}.trees",
        "{DELR_PREFIX}_parsimony.csv",
    output:
        "{DELR_PREFIX}_deletion_events.csv",
    log:
        notebook="logs/{DELR_PREFIX}_deletion-report.ipynb"
    notebook:
        "notebooks/deletion_report.py.ipynb"


rule recombinants_report:
    conda:
        "sc2ts.yml"
    input:
        "{RECR_PREFIX}.trees",
    output:
        "{RECR_PREFIX}_recombinants.csv",
    log:
        notebook="logs/{RECR_PREFIX}_recombinants_report.ipynb"
    notebook:
        "notebooks/recombinants_report.py.ipynb"



rule add_net_supporting_loci:
    conda:
        "sc2ts.yml"
    input:
        "{ASL_PREFIX}.trees",
        "{ASL_PREFIX}_recombinants_matches_pangonet.csv",
    output:
        "{ASL_PREFIX}_recombinants_matches_pangonet_nsl.csv",

    shell:
        """
        python ../scripts/run_recombinant_minlength_csv_add.py -v {input} {output}
        """

# NOTE: these two rematch steps are very slow and take a lot 
# of RAM. There's definitely ways we can speed this up, but 
# the simplest is to just let it run.
# To make this run in a tolerable amount of time we need to pass
# the previous HMM solution to run_hmm so it has a starting point.
# Otherwise we waste a lot of time generating solutions for most 
# recombinants which are actually pretty easy, but don't fully
# capture the really difficult ones (which are important).
rule rematch_arg_recombinants:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{REMARG_PREFIX}_recombinants.csv",
    output:
        "{REMARG_PREFIX}_rematch.json",
    shell:
        """
        python ../src/rematch_recombinants.py \
            ../inference/results/v1-beta1/v1-beta1_ \
            {input} {output}.tmp -k 1000 \
            --mismatch-threshold=100 \
            --workers=1
        mv {output}.tmp {output}
        """

rule rematch_non_arg_recombinants:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "../data/samples_pangos_absent_in_arg.csv",
    output:
        "pango_x_absent_in_arg_rematch.json",
    shell:      
        """
        python ../src/rematch_recombinants.py \\
            ../inference/results/v1-beta1/v1-beta1_ \
            --mismatch-threshold=100 \
            --workers=1 \
            {input} {output}.tmp -k 1000 -k 4
        mv {output}.tmp {output}
        """

# This is a temporary rule to get something that basically works
# while we let the other two above "just run". We're just covering
# the samples that changed from an earlier version because of how
# sample choice was done.
rule rematch_arg_recombinants_that_changed:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "samples_that_changed.csv",
    output:
        "samples_that_changed.json",
    shell:      
        """
        python ../src/rematch_recombinants.py \\
            ../inference/results/v1-beta1/v1-beta1_ \
            --mismatch-threshold=100 \
            --workers=1 \
            {input} {output}.tmp -k 1000 
        mv {output}.tmp {output}
        """


rule shift_breakpoints:
    conda:
        "sc2ts.yml"
    input:
        "{BP_PREFIX}.trees",
    output:
        "{BP_PREFIX}_bps.trees",
    log:
        "{BP_PREFIX}_shift_breakpoints.log",
    shell:
        """
        python ../scripts/run_breakpoint_shift_for_deletions.py -v {input} {output} > {log} 2>&1
        """


rule date:
    conda:
        "sc2ts.yml"
    input:
        "{DATE_PREFIX}.trees",
    output:
        "{DATE_PREFIX}_dated.trees",
    shell:
        """
        python ../scripts/run_nonsample_dating.py {input} {output}
        """


rule tszip:
    conda:
        "sc2ts.yml"
    input:
        "{TSZIP_PREFIX}.trees",
    output:
        "{TSZIP_PREFIX}.trees.tsz",
    shell:
        """
        python -m tszip -k {input} 
        """


rule write_fasta:
    input:
        "{WF_PREFIX}.trees",
    output:
        "{WF_PREFIX}.fasta",
    run:
        import tszip
        import tskit
        import numpy as np

        # Set all nodes to be sample nodes.
        ts = tszip.load(input[0])
        tables = ts.dump_tables()
        node_flags = tables.nodes.flags
        node_flags[:] = tskit.NODE_IS_SAMPLE
        tables.nodes.flags = node_flags
        ts = tables.tree_sequence()
        if not np.all(ts.nodes_flags == tskit.NODE_IS_SAMPLE):
            raise ValueError("Not all the nodes are samples.")
        samples = np.arange(ts.num_nodes)
        with open(f"{output}", "w") as f:
            # NOTE: this takes about 80G of RAM to do in one go. Can split
            # into chunks of nodes to make it more managable
            for j, entry in enumerate(ts.alignments(samples=samples, left=1)):
                f.write(f">n{samples[j]}\n")
                f.write(entry + "\n")


rule run_pangolin:
    conda:
        "pangolin.yml"
    input:
        "{RP_PREFIX}.fasta",
    output:
        "{RP_PREFIX}.lineage_report.csv",
    shell:
        # Keep the temp files in a local directory pangolin_tmp because we create
        # several copies of the data during the process of running it.
        """
        mkdir -p pangolin_tmp
        pangolin -t {workflow.cores} {input} --tempdir=pangolin_tmp --outfile={output}
        """


rule add_pangolin_metadata:
    input:
        "{APM_PREFIX}.trees",
        "{APM_PREFIX}.lineage_report.csv",
    output:
        "{APM_PREFIX}_pango.trees",
    run:
        import tskit
        import pandas as pd
        import tqdm

        df = pd.read_csv(input[1]).set_index("taxon")
        # Remove NaNs from missing scorpio calls
        df["scorpio_call"] = df.scorpio_call.fillna(".")
        tables = tskit.TableCollection.load(input[0])
        nodes = tables.nodes.copy()
        tables.nodes.clear()

        for u, row in enumerate(tqdm.tqdm(nodes)):
            record = df.loc[f"n{u}"]
            row.metadata["pango"] = record["lineage"]
            row.metadata["scorpio"] = record["scorpio_call"]
            assert isinstance(record["scorpio_call"], str)
            tables.nodes.append(row)

        tables.dump(f"{output}")


rule minimise_metadata_pango_scorpio:
    conda:
        "sc2ts.yml"
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mmps.trees",
    shell:
        # Convert the output of the add pangolin metadata command above
        # and drop the vestigial root edge
        """
        python -m sc2ts minimise-metadata {input} {output} \
            -m strain sample_id -m pango pango -m scorpio scorpio \
            --drop-vestigial-root
        """


rule minimise_metadata_date:
    conda:
        "sc2ts.yml"
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mmd.trees",
    shell:
        # Add the Date_tree to the minimal metadata 
        """
        python -m sc2ts minimise-metadata {input} {output} \
            -m strain sample_id -m Date_tree Date_tree
        """


rule minimise_metadata:
    conda:
        "sc2ts.yml"
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mm.trees",
    shell:
        # Minimal metadata, just keep sample_id
        """
        python -m sc2ts minimise-metadata {input} {output}
        """


########################
# Usher tree conversion
########################

rule download_json:
    output:
        "tree.all_viridian.202409.jsonl.gz"
    shell:
        """
        wget --quiet --content-disposition \
            https://figshare.com/ndownloader/files/49691040 \
            -O {output}
        """

rule download_pb:
    output:
        "tree.all_viridian.202409.pb.gz"
    shell:
        """
        wget --quiet --content-disposition \
            https://figshare.com/ndownloader/files/49691037 \
            -O {output}
        """

rule export_mutations_local:
    conda:
        # The pangolin environment above also contains a full copy
        # of Usher/matutils, so no point in redoing that.
        "pangolin.yml"
    input:
        "tree.all_viridian.202409.pb.gz"
    output:
        "usher_mutations.tsv"
    shell:
        # Got GTF from 
        # http://hgdownload.soe.ucsc.edu/goldenPath/wuhCor1/bigZips/genes/ncbiGenes.gtf.gz
        """
        matUtils summary -i tree.all_viridian.202409.pb.gz \
            -f reference.fasta -g ncbiGenes.gtf \
            --translate usher_mutations.tsv
        """

rule convert_topology:
    conda:
        "sc2ts.yml"
    input:
        "tree.all_viridian.202409.jsonl.gz"
    output:
        "usher_topology.trees"
    shell:
        "python ../src/mat2tsk.py convert-topology {input} {output}"

rule add_mutations:
    conda:
        "sc2ts.yml"
    input:
        "usher_topology.trees",
        "usher_mutations.tsv"
    output:
        "usher_v1_2024-06-06.trees"
    shell:
        """
        python ../src/mat2tsk.py convert-mutations {input} reference.fasta {output}
        """

rule date_samples:
    conda:
        "sc2ts.yml"
    input:
        # Note: call with e.g. snakemake usher_v1_2024-06-06_ds.trees
        "{DS_PREFIX}.trees",
    output:
        "{DS_PREFIX}_ds.trees",
    shell:
        """
        python ../src/mat2tsk.py date-samples {input} {output}
        """

rule date_internal_nodes:
    conda:
        "sc2ts.yml"
    input:
        # Note: call with e.g. snakemake usher_v1_2024-06-06_ds_di.trees
        "{DI_PREFIX}.trees"
    output:
        "{DI_PREFIX}_di.trees"
    shell:
        """
        echo "NOTE: tsdate can take many minutes per EP iteration: this may be a while."
        python ../src/mat2tsk.py date-internal {input} {output}
        """

rule sc2ts_usher_intersection:
    conda:
        "sc2ts.yml"
    input:
        "usher_v1_2024-06-06_mmd.trees",
        "sc2ts_v1_2023-02-21_mm.trees"
    output:
        "usher_2023-02-21_intersection.trees",
        "sc2ts_2023-02-21_intersection.trees",
    shell:
        """
        python ../src/mat2tsk.py intersect {input} {output}
        """


rule all_sites_parsimony:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{ASP_PREFIX}.trees"
    output:
        "{ASP_PREFIX}_asp.trees",
        "{ASP_PREFIX}_asp.csv"
    shell:
        """
        python -m sc2ts map-parsimony -v {input} {output[0]} \
            --report={output[1]} --no-progress
        """


rule usher_sc2ts_comparison:
    conda:
        "sc2ts.yml"
    input:
        "usher_2023-02-21_intersection.trees",
        "usher_2023-02-21_intersection_asp.trees",
        "usher_2023-02-21_intersection_asp.csv",
        "sc2ts_2023-02-21_intersection.trees",
        "sc2ts_2023-02-21_intersection_asp.trees",
        "sc2ts_2023-02-21_intersection_asp.csv",
    log:
        notebook="logs/usher_sc2ts_comparison.ipynb"
    notebook:
        "notebooks/usher_sc2ts_comparison.py.ipynb"

