# This workflow is intended to be run with the --sdm=conda option,
# and most rules require either the sc2ts or pangolin environments.
# The base conda environment that snakemake is run from requires
# snakemake, tszip, pandas and tqdm in order to run the "run"
# rules (which cannot be used with "conda")

rule all:
    input:
        # Base debug report
        "logs/sc2ts_v1_2023-02-21_debug-report.ipynb",
        # "sc2ts_v1_2023-02-21_pp_mp_aph_bps_pango_dated_mmps.csv",
        # Recombinant data
        # "sc2ts_v1_2023-02-21_pp_dated_mp_bps_pango_recombinants.csv", 
        # # Full rebar output. 
        # "sc2ts_v1_2023-02-21_pp_dated_mp_bps_pango_rebar_output.tsv",
        # # TODO add rule to generate _matches csv once we've sorted the reruns json.
        # # Currently this is computed in the recombinant_processing notebook
        # "sc2ts_v1_2023-02-21_pp_dated_mp_bps_pango_recombinants_rebar_matches_pangonet_nsl.csv",
        # # "sc2ts_v1_2023-02-21_pp_dated_mp_bps_pango_rematch.json",
        # # Report on deletion and parsimony mapping
        # "logs/sc2ts_v1_2023-02-21_pp_dated_mp_deletion-report.ipynb",
        # # Usher tree and sc2ts intersections
        # # Full usher tree with mutations, but uncalibrated node time
        # "usher_v1_2024-06-06_mm.trees.tsz",
        # # Usher intersection with sc2ts, with dated nodes
        # "usher_2023-02-21_intersection_ds_di.trees.tsz",
        # # Sc2ts intersection, not dated (annoying metdata issues)
        # "sc2ts_2023-02-21_intersection.trees.tsz",
        # # All-sites parsimony files, to get the direct comparison
        # "usher_2023-02-21_intersection_asp.trees",
        # "sc2ts_2023-02-21_intersection_asp.trees",
        # # Report
        # "logs/usher_sc2ts_comparison.ipynb",
        "logs/final_arg_report.ipynb",

rule final_args:
    conda: 
        "sc2ts.yml"
    input:
        # Final ARG
        # Order of operations:
        # 1) postprocess (_pp: various minor tweaks)
        # 2) Parsimony mapping (_mp: adding in deletions and recurrent sites)
        # 3) Apply node parsimony heuristics (_aph: cleanup some mess after 2)
        # 4) Shift breakpoints (must be after mapping: _bps)
        # 5) Compute pango assignment (must be after parsimony mapping and heuristics)
        # 6) Dating (_dated)
        # 7) Minimise metadata (must be last step)
        "sc2ts_v1_2023-02-21_pp_mp_aph_bps_pango_dated_mmps.trees.tsz",
        "usher_v1_2024-06-06_mm.trees.tsz",
        "sc2ts_2023-02-21_intersection.trees.tsz",
        "usher_2023-02-21_intersection_ds_di.trees.tsz",
    output:
        "sc2ts_viridian_v1.trees.tsz",
        "usher_viridian_v1.trees.tsz",
        "sc2ts_viridian_inter_v1.trees.tsz",
        "usher_viridian_inter_v1.trees.tsz",
    log:
        notebook="logs/final_arg_report.ipynb"
    notebook:
        "notebooks/final_arg_report.py.ipynb"
        

rule initialise:
    input:
        "../inference/results/v1-beta1/v1-beta1_{DATE}.ts",
    output:
        "sc2ts_v1_{DATE}.trees",
    shell:
        """
        cp {input} {output}
        """

rule debug_report:
    conda:
        "sc2ts.yml"
    input:
        "{DR_PREFIX}.trees",
    output:
        "{DR_PREFIX}_samples.csv",
        "{DR_PREFIX}_resources.csv",
    log:
        notebook="logs/{DR_PREFIX}_debug-report.ipynb"
    notebook:
        "notebooks/debug_report.py.ipynb"


# Add exact matches to the dataset and do some minor mutation
# rearrangements like pushing unary recombinant mutations.
rule postprocess:
    conda:
        "sc2ts.yml"
    input:
        "{PP_PREFIX}.trees"
    output:
        "{PP_PREFIX}_pp.trees",
    shell:
        """
        python -m sc2ts postprocess {input} {output} \
            --match-db=../inference/results/v1-beta1.matches.db
        """

rule map_parsimony:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{MP_PREFIX}.trees",
        "remapped_sites.txt",
    output:
        "{MP_PREFIX}_mp.trees",
        "{MP_PREFIX}_mp.csv",
    shell:
        """
        python -m sc2ts map-parsimony -vv \
            {input[0]} {input[1]} {output[0]} \
            --sites={input[2]} --report={output[1]} --no-progress
        """

rule apply_parsimony_heuristics:
    conda:
        "sc2ts.yml"
    input:
        "{APH_PREFIX}.trees"
    output:
        "{APH_PREFIX}_aph.trees",
        "{APH_PREFIX}_aph.csv",
    shell:
        """
        python -m sc2ts -vv apply-node-parsimony {input} {output[0]} \
            --report={output[1]} --no-progress
        """


rule deletion_report:
    conda:
        "sc2ts.yml"
    input:
        "{DELR_PREFIX}.trees",
        "{DELR_PREFIX}_parsimony.csv",
    output:
        "{DELR_PREFIX}_deletion_events.csv",
    log:
        notebook="logs/{DELR_PREFIX}_deletion-report.ipynb"
    notebook:
        "notebooks/deletion_report.py.ipynb"


rule recombinants_report:
    conda:
        "sc2ts.yml"
    input:
        "{RECR_PREFIX}.trees",
    output:
        "{RECR_PREFIX}_recombinants.csv",
    log:
        notebook="logs/{RECR_PREFIX}_recombinants_report.ipynb"
    notebook:
        "notebooks/recombinants_report.py.ipynb"


rule add_pangonet_to_csv:
    conda:
        "sc2ts.yml"
    input:
        "{APC_PREFIX}.csv",
    output:
        "{APC_PREFIX}_pangonet.csv",

    shell:
        """
        python scripts/add_pangonet_distance_to_csv.py {input} {output}
        """


rule add_net_supporting_loci_to_csv:
    conda:
        "sc2ts.yml"
    input:
        "{ASL_PREFIX}.trees",
        "{ASL_PREFIX}_recombinants_{ASL_SUFFIX}.csv",
    output:
        "{ASL_PREFIX}_recombinants_{ASL_SUFFIX}_nsl.csv",
    shell:
        """
        python scripts/add_recombinant_minlength_to_csv.py -v {input} {output}
        """

# NOTE: these two rematch steps are very slow and take a lot 
# of RAM. There's definitely ways we can speed this up, but 
# the simplest is to just let it run.
# To make this run in a tolerable amount of time we need to pass
# the previous HMM solution to run_hmm so it has a starting point.
# Otherwise we waste a lot of time generating solutions for most 
# recombinants which are actually pretty easy, but don't fully
# capture the really difficult ones (which are important).
rule rematch_arg_recombinants:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{REMARG_PREFIX}_recombinants.csv",
    output:
        "{REMARG_PREFIX}_rematch.json",
    shell:
        """
        python scripts/rematch_recombinants.py \
            ../inference/results/v1-beta1/v1-beta1_ \
            {input} {output}.tmp -k 1000 \
            --mismatch-threshold=100 \
            --workers=1
        mv {output}.tmp {output}
        """

rule rematch_non_arg_recombinants:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "../data/samples_pangos_absent_in_arg.csv",
    output:
        "pango_x_absent_in_arg_rematch.json",
    shell:      
        """
        python scripts/rematch_recombinants.py \\
            ../inference/results/v1-beta1/v1-beta1_ \
            --mismatch-threshold=100 \
            --workers=1 \
            {input} {output}.tmp -k 1000 -k 4
        mv {output}.tmp {output}
        """


rule shift_breakpoints:
    conda:
        "sc2ts.yml"
    input:
        "{BP_PREFIX}.trees",
    output:
        "{BP_PREFIX}_bps.trees",
    log:
        "{BP_PREFIX}_shift_breakpoints.log",
    shell:
        """
        python scripts/run_breakpoint_shift_for_deletions.py -v {input} {output} > {log} 2>&1
        """


rule date:
    conda:
        "sc2ts.yml"
    input:
        "{DATE_PREFIX}.trees",
    output:
        "{DATE_PREFIX}_dated.trees",
    shell:
        """
        python scripts/run_nonsample_dating.py {input} {output}
        """


rule tszip:
    conda:
        "sc2ts.yml"
    input:
        "{TSZIP_PREFIX}.trees",
    output:
        "{TSZIP_PREFIX}.trees.tsz",
    shell:
        """
        python -m tszip -k {input} 
        """


rule write_fasta:
    conda:
        "sc2ts.yml"
    input:
        "{WF_PREFIX}.trees",
    output:
        "{WF_PREFIX}.fasta",

    shell:
        """
        python scripts/get_all_nodes_fasta.py {input} {output}
        """

rule run_pangolin:
    conda:
        "pangolin.yml"
    input:
        "{RP_PREFIX}.fasta",
    output:
        "{RP_PREFIX}.lineage_report.csv",
    shell:
        # Keep the temp files in a local directory pangolin_tmp because we create
        # several copies of the data during the process of running it.
        """
        mkdir -p pangolin_tmp
        pangolin -t {workflow.cores} {input} --tempdir=pangolin_tmp --outfile={output}
        """


rule add_pangolin_metadata:
    input:
        "{APM_PREFIX}.trees",
        "{APM_PREFIX}.lineage_report.csv",
    output:
        "{APM_PREFIX}_pango.trees",
    run:
        import tskit
        import pandas as pd
        import tqdm

        df = pd.read_csv(input[1]).set_index("taxon")
        # Remove NaNs from missing scorpio calls
        df["scorpio_call"] = df.scorpio_call.fillna(".")
        tables = tskit.TableCollection.load(input[0])
        nodes = tables.nodes.copy()
        tables.nodes.clear()

        for u, row in enumerate(tqdm.tqdm(nodes)):
            record = df.loc[f"n{u}"]
            row.metadata["pango"] = record["lineage"]
            row.metadata["scorpio"] = record["scorpio_call"]
            assert isinstance(record["scorpio_call"], str)
            tables.nodes.append(row)

        tables.dump(f"{output}")


rule minimise_metadata_pango_scorpio:
    conda:
        "sc2ts.yml"
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mmps.trees",
    shell:
        # Convert the output of the add pangolin metadata command above
        # and drop the vestigial root edge
        """
        python -m sc2ts minimise-metadata {input} {output} \
            -m strain sample_id -m pango pango -m scorpio scorpio \
            --drop-vestigial-root
        """


rule minimise_metadata_date:
    conda:
        "sc2ts.yml"
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mmd.trees",
    shell:
        # Add the Date_tree to the minimal metadata 
        """
        python -m sc2ts minimise-metadata {input} {output} \
            -m strain sample_id -m Date_tree Date_tree
        """


rule minimise_metadata:
    conda:
        "sc2ts.yml"
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mm.trees",
    shell:
        # Minimal metadata, just keep sample_id
        """
        python -m sc2ts minimise-metadata {input} {output}
        """


########################
# Usher tree conversion
########################

rule download_json:
    output:
        "tree.all_viridian.202409.jsonl.gz"
    shell:
        """
        wget --quiet --content-disposition \
            https://figshare.com/ndownloader/files/49691040 \
            -O {output}
        """

rule download_pb:
    output:
        "tree.all_viridian.202409.pb.gz"
    shell:
        """
        wget --quiet --content-disposition \
            https://figshare.com/ndownloader/files/49691037 \
            -O {output}
        """

rule export_mutations_local:
    conda:
        # The pangolin environment above also contains a full copy
        # of Usher/matutils, so no point in redoing that.
        "pangolin.yml"
    input:
        "tree.all_viridian.202409.pb.gz"
    output:
        "usher_mutations.tsv"
    shell:
        # Got GTF from 
        # http://hgdownload.soe.ucsc.edu/goldenPath/wuhCor1/bigZips/genes/ncbiGenes.gtf.gz
        """
        matUtils summary -i tree.all_viridian.202409.pb.gz \
            -f reference.fasta -g ncbiGenes.gtf \
            --translate usher_mutations.tsv
        """

rule convert_topology:
    conda:
        "sc2ts.yml"
    input:
        "tree.all_viridian.202409.jsonl.gz"
    output:
        "usher_topology.trees"
    shell:
        "python scripts/mat2tsk.py convert-topology {input} {output}"

rule add_mutations:
    conda:
        "sc2ts.yml"
    input:
        "usher_topology.trees",
        "usher_mutations.tsv"
    output:
        "usher_v1_2024-06-06.trees"
    shell:
        """
        python scripts/mat2tsk.py convert-mutations {input} reference.fasta {output}
        """

rule date_samples:
    conda:
        "sc2ts.yml"
    input:
        # Note: call with e.g. snakemake usher_v1_2024-06-06_ds.trees
        "{DS_PREFIX}.trees",
    output:
        "{DS_PREFIX}_ds.trees",
    shell:
        """
        python scripts/mat2tsk.py date-samples {input} {output}
        """

rule date_internal_nodes:
    conda:
        "sc2ts.yml"
    input:
        # Note: call with e.g. snakemake usher_v1_2024-06-06_ds_di.trees
        "{DI_PREFIX}.trees"
    output:
        "{DI_PREFIX}_di.trees"
    shell:
        """
        echo "NOTE: tsdate can take many minutes per EP iteration: this may be a while."
        python scripts/mat2tsk.py date-internal {input} {output}
        """

rule sc2ts_usher_intersection:
    conda:
        "sc2ts.yml"
    input:
        "usher_v1_2024-06-06_mmd.trees",
        "sc2ts_v1_2023-02-21_pp_mm.trees"
    output:
        "usher_2023-02-21_intersection.trees",
        "sc2ts_2023-02-21_intersection.trees",
    shell:
        """
        python scripts/mat2tsk.py intersect --intersect-sites {input} {output}
        """


rule generate_all_sites:
    output: "all_sites.txt"
    shell: "seq 1 29903 > {output}"


rule all_sites_parsimony:
    conda:
        "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{ASP_PREFIX}.trees",
        "all_sites.txt",
    output:
        "{ASP_PREFIX}_asp.trees",
        "{ASP_PREFIX}_asp.csv"
    shell:
        """
        python -m sc2ts map-parsimony -v {input[0]} {input[1]} {output[0]} \
            --report={output[1]} --no-progress \
            --sites={input[2]}
        """


rule usher_sc2ts_comparison:
    conda:
        "sc2ts.yml"
    input:
        "usher_2023-02-21_intersection.trees",
        "usher_2023-02-21_intersection_asp.trees",
        "usher_2023-02-21_intersection_asp.csv",
        "sc2ts_2023-02-21_intersection.trees",
        "sc2ts_2023-02-21_intersection_asp.trees",
        "sc2ts_2023-02-21_intersection_asp.csv",
    log:
        notebook="logs/usher_sc2ts_comparison.ipynb"

    notebook:
        "notebooks/usher_sc2ts_comparison.py.ipynb"


rule rebar_dataset:
    conda: "rebar.yml"
    output: "rebar/summary.json"
    shell:
      """
      rebar dataset download \
        --name sars-cov-2 \
        --tag 2025-01-28 \
        --verbosity error \
        --output-dir rebar
      """


rule run_rebar:
    conda: "rebar.yml"
    input: 
        "rebar/summary.json",
        "{RR_PREFIX}_recombinants.fasta"
    output:
        "{RR_PREFIX}_rebar_output.tsv"
    shell:
      """
      rebar run \
          --dataset-dir rebar \
          --threads {workflow.cores} \
          --alignment {input[1]} \
          --output-dir rebar_output
      cp rebar_output/linelist.tsv {output}
      """
        
rule extract_recombinant_alignments:
    conda: "sc2ts.yml"
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{ERA_PREFIX}_recombinants.csv",
    output:
        "{ERA_PREFIX}_recombinants.fasta",
    shell:
        """
        python scripts/get_sample_fasta.py {input} {output}
        """


rule add_rebar_to_csv:
    conda: "sc2ts.yml"
    input: 
        "{ARC_PREFIX}_recombinants.csv",
        "{ARC_PREFIX}_rebar_output.tsv",
    output:
        "{ARC_PREFIX}_recombinants_rebar.csv",
    shell:
        """
        python scripts/add_rebar_to_csv.py {input} {output}
        """
