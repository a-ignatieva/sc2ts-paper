
rule all:
    input:
        "sc2ts_v1_2020-06-21_md-0.01_bps_pango_dated_mm.trees.tsz",
        #"sc2ts_v1_2023-02-21_md-0.01_bps_dated.trees.tsz",


INFERENCE_DIR = "../inference/results/"


# Add exact matches to the dataset and do some minor mutation
# rearrangements like pushing unary recombinant mutations.
rule postprocess:
    input:
        "../inference/results/v1-beta1/v1-beta1_{DATE}.ts",
    output:
        "sc2ts_v1_{DATE}.trees",
    shell:
        """
        python -m sc2ts postprocess {input} {output} \
            --match-db=../inference/results/v1-beta1.matches.db
        """


# Map the deletions from the alignments back into the trees,
# considering only deletions above given frequency threshold.
rule map_deletions:
    input:
        "../data/viridian_mafft_2024-10-14_v1.vcz.zip",
        "{MD_PREFIX}.trees",
    output:
        r"{MD_PREFIX}_md-{DEL_FREQ,0.\d+}.trees",
    shell:
        """
        python -m sc2ts map-deletions {input} {output} \
            --frequency-threshold={wildcards.DEL_FREQ}
        """


rule shift_breakpoints:
    input:
        "{BP_PREFIX}.trees",
    output:
        "{BP_PREFIX}_bps.trees",
    shell:
        """
        python ../scripts/run_breakpoint_shift_for_deletions.py {input} {output}
        """


rule date:
    input:
        "{DATE_PREFIX}.trees",
    output:
        "{DATE_PREFIX}_dated.trees",
    shell:
        """
        python ../scripts/run_nonsample_dating.py {input} {output}
        """


# ruleorder: date > add_pangolin_metadata > shift_breakpoints > map_deletions


rule tszip:
    input:
        "{TSZIP_PREFIX}.trees",
    output:
        "{TSZIP_PREFIX}.trees.tsz",
    shell:
        """
        python -m tszip -k {input} 
        """


rule write_fasta:
    input:
        "{WF_PREFIX}.trees",
    output:
        "{WF_PREFIX}.fasta",
    run:
        import tszip
        import tskit
        import numpy as np

        # Set all nodes to be sample nodes.
        ts = tszip.load(input[0])
        tables = ts.dump_tables()
        node_flags = tables.nodes.flags
        node_flags[:] = tskit.NODE_IS_SAMPLE
        tables.nodes.flags = node_flags
        ts = tables.tree_sequence()
        if ~np.all(ts.nodes_flags == tskit.NODE_IS_SAMPLE):
            raise ValueError("Not all the nodes are samples.")
        samples = np.arange(ts.num_nodes)
        with open(f"{output}", "w") as f:
            # NOTE: this takes about 80G of RAM to do in one go. Can split
            # into chunks of nodes to make it more managable
            for j, entry in enumerate(ts.alignments(samples=samples, left=1)):
                f.write(f">n{samples[j]}\n")
                f.write(entry + "\n")


rule run_pangolin:
    input:
        "{RP_PREFIX}.fasta",
    output:
        "{RP_PREFIX}.lineage_report.csv",
    conda:
        "pangolin.yml"
    shell:
        # Keep the temp files in a local directory pangolin_tmp because we create
        # several copies of the data during the process of running it.
        """
        mkdir -p pangolin_tmp
        pangolin -t {threads} {input} --tempdir=pangolin_tmp --outfile={output}
        """


rule add_pangolin_metadata:
    input:
        "{APM_PREFIX}.trees",
        "{APM_PREFIX}.lineage_report.csv",
    output:
        "{APM_PREFIX}_pango.trees",
    run:
        import tskit
        import pandas as pd
        import tqdm

        df = pd.read_csv(input[1]).set_index("taxon")
        tables = tskit.TableCollection.load(input[0])
        nodes = tables.nodes.copy()
        tables.nodes.clear()

        for u, row in enumerate(tqdm.tqdm(nodes)):
            record = df.loc[f"n{u}"]
            row.metadata["pango"] = record["lineage"]
            row.metadata["scorpio"] = record["scorpio_call"]
            tables.nodes.append(row)

        tables.dump(f"{output}")


rule minimise_metadata:
    input:
        "{MM_PREFIX}.trees",
    output:
        "{MM_PREFIX}_mm.trees",
    shell:
        """
        python -m  sc2ts minimise-metadata --pango-field=pango {input} {output}
        """
